\documentclass{article}
% setup page
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage[a4paper, top=2.5cm, left=2cm, right=2cm, bottom=2cm]{geometry}
\usepackage{titling}

% for random text
\usepackage{lipsum}

\usepackage{todonotes}
\usepackage{amsmath}

\usepackage[backend=bibtex]{biblatex}
\addbibresource{papers.bib}

% setup info
\title{IR Project 1: Report}
\author{Tobias Veto, Philip Junker, Marc Fischer}

% setup header/footer
\pagestyle{fancy}
\fancyhf{}
\rhead{\theauthor}
\lhead{\thetitle}
\rfoot{Page \thepage}
 

\begin{document}

\section*{TODO}
\begin{itemize}
	\item result files
	\item readme file
	\item package code and results
	\item finish report
	\item clean up bibtex notes
\end{itemize}

 
\section*{Introduction}
For the implementations of the naive bayes, logistic regession and SVM we stayed close to the code shown in the lecture. Thus the implementations of the algorithms were straightforward while optimization and data preprocessing were challenging. Most of our effort went into the data pruning.


\section*{Data transformation and pruning}
A big part of our implementation was data preprocessing. We read the tokens from the documents (with tinyIR), remove tokens not consiting of letters, stem the word and remove stopwords. \cite{joachims_text_1998,ozgur_text_2005} both list this as the standard approach to preprocessing in document classifcation. From the resulting corpus of documents we remove those that occur less then \texttt{minOccurrence} and those that are found in more than $\text{\texttt{maxOccurrenceRate}} \cdot \text{\texttt{nrDocuments}}$, where $\text{\texttt{maxOccurrenceRate}} \in (0, 1]$.
The resulting dictionary of words is uses to rerpresent each document as a bag-of-words vector. For the weights within the vector we use boolean weights (1 if the word occurs in the document, else 0), tf-idf weights (insprired by \cite{ozgur_text_2005}) and the count of word.  \todo{specify weighting method here} has generally yielded the best result.
We also discovered, that words extracted from the title of the document contain a high amount of information and lend themselves especially to predicting country codes. The words from the headers were processed like the words from the content.

\section*{Naive Bayes}
\lipsum[1-4] %replace with real text

\section*{Logistic Regression}
\lipsum[1-4] %replace with real text

\section*{SVM}
The algorithm shown in the lecture is the well-known pegsasos algorithm\cite{shalev-shwartz_pegasos:_2011,shalev-shwartz_pegasos:_????}. For the implementation we train one SVM per category occuring in the training data in an all-vs-one approach.
Data was preproccessed as introduced before with the use of homogeneous coordinates (bias term) and without. The listed papers suggest, not using a bias with the normal form of the pegasos algorith or to adjust it if doing so, as it undermines the convergences guarantee. \todo{results form bias}
The results form \cite{joachims_text_1998} strongly suggest to use a polynomial or RBF kernel, but \todo{RBF on title}.


\printbibliography

\end{document}